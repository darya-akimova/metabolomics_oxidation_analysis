{
    "collab_server" : "",
    "contents" : "---\ntitle: \"And example of a metabolomics untargeted analysis project: Precolumn Oxidation\"\nauthor: \"Darya Akimova\"\ndate: \"August 7, 2017\"\noutput: html_document\n---\n\nNote: This is older data - I removed the meaningful identifiers from the compounds (Mass, LC Retention Time) and all compounds are only referenced by an arbitrary number (although the compounds were sorted by mass from smallest to largest before being given a number).\n\n\n### Background:\n\n#### The Motivation\n\nThis is a mass spectrometry metabolomics experiment based on a project by a post-doc in my lab, who is in the process of creating a database of oxidation-senstive molecules, as well as the products of oxidation. \n\nThe motivation behind this project is that there is evidence that a variety of diseases may be caused by oxidative stress. Oxidative stress occurs when there is a large quantity of oxidizing molecules present in a cell, tissue, or organism, such that cannot be neautralized by the organism's antioxidant defenses, which come in a variety of forms and can be produced by the organism itself or obtained from the diet. When oxidative stress is too high, the oxidizing molecules go rogue, chemically speaking, and can damage DNA, proteins, and other molecules within the cell. This type of cellular damage has been linked to a variety of diseases, including cancer and heart disease. Therefore, in order to understand and treat these diseases properly, we need to understand the nature of the oxidative stress. However, we have a limited understanding of all of the molecules that are oxidation-senstive, and what molecules might be products of oxidative stress. \n\n#### The Project\n\nTo expand our knowledge about oxidative stress, a project has been initiated in my lab to create a \"redoxome\" database, which is a database of redox-related molecules, and that database should be ideally specific to the biological fluid or tissue of interest. The basic idea is to generate a database of \"real\" oxidation-sensitive molecules and the molecules produced by oxidative stress by taking a normal sample of the biological material that one is interested in studying, and subjecting it to one or more oxidizing potentials in a chemical oxidation cell (an artificially oxidizing environment). \n\nIn the case of this specific experient, there is a base/control \"0mV\" fraction, and 4 oxidized fractions, \"250mV\", \"500mV\", \"750mV\", and \"1000mV\". The fractions were run on our LC/MS system to quantify the abundances of the various molecules present in each sample. The features were then extracted in an \"untargeted\" way - as in it's possible to extract features based only on compound mass and retention time on the LC system, without knowing their exact identity. \n\nGenerally, it's a challenge to identify compounds based only on mass and retention time alone, especially as mass increases, because the potential chemical space increases and the ways that the atoms that make up that molecule can be connected increases. One way to identify compounds is to use a database, most labs will have their own database of compounds that they've run on their own system (while mass does not vary, retention time will vary with chromatography, and the separation of various isoforms will vary), or labs will adopat a chromatography system matched to a particular database. Identifying compounds based on mass is the next best thing, but it's never a sure thing.\n\nUnknowns can be subjected to what is known as MS/MS for identification - certain MS instruments can be used to subject samples to two rounds of MS. The first MS step is to select a peak or feature of interest, the second MS step is to fragment the compound. MS/MS is done on each compound of interest one by one to generate a clean fragment spectrum for each compound. The compound structure can then be guessed at because each molecule will break up in particular ways depending on which bonds are the most susceptible to breaking. Even when a compound structure is proposed, a standard is usually bought or synthesized (usually bought, because the standard has to be MS-grade purity with no potential confounding compounds), and the standard is run on the LC/MS system to compare retention time and MS/MS is performed to make sure the fragmentation patterns of the unknown and the standard are a match. \n\nHopefully it's becoming clear that identifying a potentially interesting unknown is a lot of work and potententially expensive. Therefore, it's extremely important to select well!\n\n#### The Goals\n\n1) Find potentially interesting biomarkers of oxidative stress.\n2) Create a model that could be used to estimate the oxidative stress in \"Potential (mV)\" units in a real biological sample, given only the compounds present in that sample and their abundances.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE)\n```\n\n\n```{r packages}\n# packages\nrequire(heatmaply)\nrequire(magrittr)\nrequire(tidyverse)\nrequire(cowplot)\n```\n\nAnother note that I should make, before getting too far, is that a Mass Spec can only detect charged molecules (as in a molecule with one or more total positive or negative charges). A MS instrument cannot measure an uncharged netural molecule and there is a number of ways to 1) give molecules a charge and 2) to detect the charged molcule. Also, there are two detection modes: \n\n+ Negative, which is better for picking up the signals of molecules that have a tendency to accept a negative charge\n+ Positive, in which the instrument will only pick up signals from positively charged molecules, and is therefore better for quantifying the abundances of molecules that tend to pick up positive charges\n\nSome molecules can be picked up in both modes, others will be detected exclusively in one, but not the other. Depending on whether the quantification is relative or absolute (using a standard curve), the values for the same molecule in the two modes may not agree because molecules tend to be better at picking up one type of charge over another.\n\nThe set of samples considered in this analysis was analyzed in both negative and positive mode, and both datasets are analyzed here. n._ will denote negative mode data, whereas p._ will denote positive mode data.\n\n```{r}\n# data\nn.data <- read_csv(file = \"QTOF_precolumn_redox_neg_untarget_nomassRT.csv\", col_names = TRUE)\np.data <- read_csv(file = \"QTOF_precolumn_redox_pos_untarget_nomassRT.csv\", col_names = TRUE)  \n\nncol(n.data)\nncol(p.data)\n```\n\nThere are 946 Compounds (Features) associated with the negative mode samples (948 columns in the n.data because 1 column is sample names and the other contains the oxidizing potential measurement). There are 474 Compounds associated with the postive mode data. Important note: even though the same compound may be detected in both and negative mode, this is not always the case, and the nC1 is not necessarily the same as pC1 (and so on for the other numbered compounds). \n\nAnother quick note: n.X stands for negative mode data and p.X stands for positive mode data.\n\nWhat the data looks like:\n\n```{r data_show}\nn.data\nsummary(n.data$Potential)\n```\n\nThere are 5 samples per each oxidizing potential.\n\n```{r factor_potential}\nn.data$Potential <- factor(n.data$Potential, \n                           levels = c(0, 250, 500, 750, 1000), \n                           labels = c(\"0\", \"250\", \"500\", \"750\", \"1000\"))\np.data$Potential <- factor(p.data$Potential, \n                           levels = c(0, 250, 500, 750, 1000), \n                           labels = c(\"0\", \"250\", \"500\", \"750\", \"1000\"))\n```\n  \nMetabolomics data tend to have a right-skewed distribution and it's customary to log-transform the data. PCA is senstive to extreme values and so here, I will log-transform as well before the PCA analysis.\n\n\n```{r log_trans_and_pca}\nn.data.log <- bind_cols(\n  select(n.data, Samples:Potential),\n  n.data %>% select(nC1:nC946) %>% log()\n  )\np.data.log <- bind_cols(\n  select(p.data, Samples:Potential),\n  p.data %>% select(pC1:pC474) %>% log()\n)\n```\n\n```{r}\n#### Clustering to show data ###\nn.data.log %>%\n  select(nC1:nC946) %>%\n  as.matrix() %>%\n  `rownames<-`(n.data.log$Samples) %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  heatmap(scale = \"none\")\n```\n\n```{r}\np.data.log %>%\n  select(pC1:pC474) %>%\n  as.matrix() %>%\n  `rownames<-`(p.data.log$Samples) %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  heatmap(scale = \"none\")\n```\n\nI normally use mixOmics::cim() to create heatmaps, but it's not agreeing with me. Trying out heatmap for now, will fix the terrible color scheme later.\n\n```{r pca_calc}\nn.pca <- prcomp(n.data.log[, 3:ncol(n.data.log)], center = TRUE, scale = FALSE)\np.pca <- prcomp(p.data.log[, 3:ncol(p.data.log)], center = TRUE, scale = FALSE)\n\n```\n\n\n\n```{r var_explained}\nn.prop.var <- data.frame(\n  cbind(\n    1:length(n.pca$sdev),\n    n.pca$sdev^2 / sum(n.pca$sdev^2),\n    cumsum(n.pca$sdev^2 / sum(n.pca$sdev^2))\n    )\n  )\ncolnames(n.prop.var) <- c(\"PC\", \"Proportion\", \"Cumulative\")\n\np.prop.var <- data.frame(\n  cbind(\n    1:length(p.pca$sdev), \n    p.pca$sdev^2 / sum(p.pca$sdev^2),\n    cumsum(p.pca$sdev^2 / sum(p.pca$sdev^2))\n    )\n  )\ncolnames(p.prop.var) <- c(\"PC\", \"Proportion\", \"Cumulative\")\n\n# Proportion of variance and cumularive variance plots, to select number of principal components\n\nn.prop.var %>%\n  ggplot(aes(x = PC, y = Proportion)) + \n  geom_point(size = 3) +\n  geom_line(size = 1.5) +\n  xlab(\"Principal Component\") +\n  ylab(\"Proportion of Variance Explained\") +\n  ggtitle(\"Negative mode \\n Proportion of Variance\")\n\nn.prop.var %>%\n  ggplot(aes(x = PC, y = Cumulative)) + \n  geom_point(size = 3) +\n  geom_line(size = 1.5) +\n  xlab(\"Principal Component\") +\n  ylab(\"Cumulative Proportion of Variance Explained\") +\n  ggtitle(\"Negative mode \\n Cumulative Proportion of Variance\")\n\np.prop.var %>% \n  ggplot(aes(x = PC, y = Proportion)) + \n  geom_point(size = 3) +\n  geom_line(size = 1.5) +\n  xlab(\"Principal Component\") +\n  ylab(\"Proportion of Variance Explained\") +\n  ggtitle(\"Positive mode \\n Proportion of Variance\")\n\np.prop.var %>% \n  ggplot(aes(x = PC, y = Cumulative)) + \n  geom_point(size = 3) +\n  geom_line(size = 1.5) +\n  xlab(\"Principal Component\") +\n  ylab(\"Cumulative Proportion of Variance Explained\") +\n  ggtitle(\"Positive mode \\n Cumulative Proportion of Variance\")\n```\n\n#Figure out which PCs are important for separating out the data\n````{r neg_comp_plots}\nn.components <- n.data %>%\n  select(Samples:Potential) %>%\n  bind_cols(data.frame(n.pca$x))\n  \nn.components %>%\n  ggplot(aes(x = PC1, y = PC2, group = Potential, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC1 vs PC2 \\n Negative Mode\") +\n  xlab(\"PC1 (77.3% var)\") +\n  ylab(\"PC2 (12.9% var)\") +\n  ylim(-7.0, 7.0)\n\nn.components %>%\n  ggplot(aes(x = PC2, y = PC3, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC2 vs PC3 \\n Negative Mode\") +\n  xlab(\"PC2 (12.9% var)\") +\n  ylab(\"PC3 (2.96%)\") +\n  xlim(-7.0, 7.0) +\n  ylim(-3.5, 3.5)\n\nn.components %>%\n  ggplot(aes(x = PC3, y = PC4, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC3 vs PC4 \\n Negative Mode\")  +\n  xlab(\"PC3 (2.96%)\") +\n  ylab(\"PC4 (1.98%)\") +\n  xlim(-3.5, 3.5) +\n  ylim(-3.0, 3.0)\n\nn.comp.num <- as.data.frame(\n  cbind(\n    as.numeric(as.character(n.components$Potential)), \n    n.components$PC1\n    )\n  )\ncolnames(n.comp.num) <- c(\"Potential\", \"PC1\")\nrow.names(n.comp.num) <- n.components$Samples\n\nn.PC1.model <- lm(PC1 ~ Potential, data = n.comp.num)\nsummary(n.PC1.model)\n\nn.comp.num %>%\n  ggplot(aes(x = Potential, y = PC1, color = Potential)) +\n  geom_point(size = 3.5) +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  ggtitle(\"PC1 vs Potential (mV)\\nNegative Mode\") +\n  xlab(\"Potential (mV)\")\n```\n  \n  \n  \n```{r pos_comp_plots}\np.components <- p.data %>%\n  select(Samples:Potential) %>%\n  bind_cols(data.frame(p.pca$x))\n  \np.components %>%\n  ggplot(aes(x = PC1, y = PC2, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC1 vs PC2 \\n Positive Mode\") +\n  xlab(\"PC1 (77.1%)\") +\n  ylab(\"PC2 (12.7%)\") +\n  ylim(-4, 4) +\n  xlim(-12.5, 12.5)\n\np.components %>%\n  ggplot(aes(x = PC2, y = PC3, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC2 vs PC3 \\n Positive Mode\") +\n  xlab(\"PC2 (12.7%)\") +\n  ylab(\"PC3 (2.93%)\") +\n  xlim(-4.5, 4.5) +\n  ylim(-3, 3)\n  \np.components %>%\n  ggplot(aes(x = PC3, y = PC4, color = Potential)) +\n  geom_point(size = 3.5) +\n  ggtitle(\"PC3 vs PC4 \\n Positive Mode\") +\n  xlab(\"PC3 (2.93%)\") +\n  ylab(\"PC4 (1.52%)\") +\n  xlim(-3, 3) +\n  ylim(-3, 3)\n\np.comp.num <- as.data.frame(\n  cbind(\n    as.numeric(as.character(p.components$Potential)), \n    p.components$PC1\n    )\n  )\ncolnames(p.comp.num) <- c(\"Potential\", \"PC1\")\nrow.names(p.comp.num) <- p.components$Samples\n\np.PC1.model <- lm(PC1 ~ Potential, data = p.comp.num)\nsummary(p.PC1.model)\n\np.comp.num %>%\n  ggplot(aes(x = Potential, y = PC1, color = Potential)) +\n  geom_point(size = 3.5) +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  ggtitle(\"PC1 vs Potential (mV)\\nPositive Mode\") +\n  xlab(\"Potential (mV)\")\n```\n\n\nNote that the separation that makes the most sense is along PC1 (0mV to 1000mV spread out along the PC1 axis). PC2 may be interesting, but confusing, as there is some separation between groups, but 0mV and 1000mV are nearly at the same position, 250mV and 750mV are very similar, and 500mV is different from all other groups.PC3 may inform how the 0mV and the 250mV groups differ from the other three groups (and from each other).\n\n```{r neg_PC1_load}\nn.PC1.load <- tbl_df(n.pca$rotation) %>%\n  mutate(Compounds = row.names(data.frame(n.pca$rotation))) %>%\n  select(Compounds, PC1) %>%\n  arrange(PC1)\n\nn.PC1.load %>%  \n  ggplot(aes(sample = PC1)) + \n  stat_qq() +\n  ggtitle(\"QQ Plot\\nNegative Mode PC1\") +\n  ylim(-0.4, 0.4)\n\n#take the bottom 2.5% of loadings and the top 2.5% of loadings\nn.select.PC1 <- subset(\n  n.PC1.load, \n  PC1 >= quantile(n.PC1.load$PC1, 0.975) | \n    PC1 <= quantile(n.PC1.load$PC1, 0.025)\n  )\n\nn.select.PC1\n\n# These compounds are much more manageable to plot in a heatmap figure\n\nn.data.log %>%\n  select(one_of(n.select.PC1$Compounds)) %>%\n  as.matrix() %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  `rownames<-`(n.data.log$Samples) %>%\n  heatmaply(scale = \"none\")\n```\n\n\n\nInteresting compounds based on pos mode PC1. \n\n```{r}\np.PC1.load <- tbl_df(p.pca$rotation) %>%\n  mutate(Compounds = row.names(data.frame(p.pca$rotation))) %>%\n  select(Compounds, PC1) %>%\n  arrange(PC1)\n  \np.PC1.load %>% \n  ggplot( aes(sample = PC1)) + \n  stat_qq() +\n  ggtitle(\"QQ Plot\\nPositive Mode PC1\") +\n  ylim(-0.3, 0.3)\n\n  \np.select.PC1 <- subset(\n  p.PC1.load,\n  PC1 >= quantile(p.PC1.load$PC1, 0.975) |\n    PC1 <= quantile(p.PC1.load$PC1, 0.025)\n  )\n\np.select.PC1\n\np.data.log %>%\n  select(one_of(p.select.PC1$Compounds)) %>%\n  as.matrix() %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  `rownames<-`(p.data.log$Samples) %>%\n  heatmaply(scale = \"none\")\n```\n\nWhat to do after this point?\n\n1) Find which compounds overlap between pos and neg mode? Eliminate repeats\n",
    "created" : 1502658268598.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2418363217",
    "id" : "68F2655B",
    "lastKnownWriteTime" : 1502660603,
    "last_content_update" : 1502660603336,
    "path" : "C:/Users/Dasha/Desktop/R_Projects/metabolomics_oxidation_analysis/QTOF_precolumn_redox_untarget_analysis_and_plots_modified.rmd",
    "project_path" : "QTOF_precolumn_redox_untarget_analysis_and_plots_modified.rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}